<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

  <title>Iaroslav Ponomarenko</title>

  <meta name="author" content="Yaroslav Ponomarenko" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" href="assets/stylesheet.css" />
</head>

<body>
  <table style="
    width: 100%;
    max-width: 794px;
    min-width: 794px;
    border: none;
    border-spacing: 0;
    border-collapse: separate;
    margin: 0 auto;
    ">
    <tbody>
      <tr style="padding: 0;">
        <td style="padding: 0;">
          <table style="width: 100%; border: none; border-spacing: 0; border-collapse: separate; margin: 0 auto;">
            <tbody>
              <tr style="padding: 0;">
                <td style="padding: 2.5%; width: 63%; vertical-align: middle;">
                  <h2 class="name" style="text-align: center;">
                    Iaroslav Ponomarenko
                  </h2>

                  <!-- <p style="text-align: center;">
                    yaroslav [dot] ponomarenko [at] stu [dot] pku [dot] edu
                    [dot] cn
                  </p> -->

                  <p>
                    I am a second-year master's student at
                    <a href="https://cs.pku.edu.cn" target="_blank">Peking University</a>'s
                    <a href="https://cfcs.pku.edu.cn/people/graduate_students/index.htm" target="_blank">
                      Center on Frontiers of Computing Studies</a>, where I am fortunate to work
                    with Professor
                    <a href="https://zsdonghao.github.io" target="_blank">Hao Dong</a>
                    at the joint
                    <a href="https://cfcs.pku.edu.cn/english/research/researchlabs/237027.htm" target="_blank">
                      Hyperplane/AGIBot PKU Lab</a>
                    on embodied AI, robotics, large foundation models, and
                    computer vision. Additionally, I am a research intern at
                    <a href="https://agibot.com" target="_blank">AGIBot</a>.
                  </p>

                  <p>
                    Previously, I obtained an Engineering degree in
                    Information Systems and Technologies from
                    <a href="https://vivt.ru" target="_blank">Voronezh Institute of High
                      Technologies</a>, as well as a Technician degree in Automated Information
                    Processing and Control Systems from Borisoglebsk College
                    of Informatics and Computer Engineering.
                  </p>

                  <p style="text-align: center">
                    <!-- <a href="assets/data/YaroslavPonomarenko-CV.pdf" target="_blank">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="assets/data/YaroslavPonomarenko-bio.txt" target="_blank">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?hl=en&amp;user=bBFYNasAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target="_blank">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/yaroslavponomarenko/" target="_blank">LinkedIn</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/YaroslavPonomarenko" target="_blank">GitHub</a>
                  </p>
                </td>
                <td style="
                padding: 2.5%;
                width: 40%;
                max-width: 40%;
                ">
                  <img src="assets/data/images/IaroslavPonomarenko.png" alt="Iaroslav Ponomarenko" style="width: 100%; max-width: 100%; object-fit: cover; border-radius: 50%;" />
                  <br>
                  <div style="text-align: center;">
                    <a href="https://hits.seeyoufarm.com"><img
                        src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fyaroslavponomarenko.github.io&count_bg=%2300AAFF&title_bg=%23555555&icon=apacherocketmq.svg&icon_color=%23FFFFFF&title=t%2B1&edge_flat=false" /></a>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="
          width: 100%;
          border: none;
          border-spacing: 0;
          border-collapse: separate;
          margin: 0 auto;
          ">
            <tbody>
              <tr>
                <td style="padding: 2.5%; width:100%; vertical-align:middle">
                  <h2>News</h2>
                  <table style="width: 100%; border-spacing: 10px;">
                    <tr>
                      <td style="width: 70px; vertical-align: middle;">
                        2024-10-17
                      </td>
                      <td>
                        Presented <strong>ManipVQA</strong> [<a href="#2">2</a>] at
                        <a href="http://iros2024-abudhabi.org" target="_blank">IROS 2024</a>
                        (Abu Dhabi, United Arab Emirates).
                      </td>
                    </tr>
                    <tr>
                      <td style="width: 70px; vertical-align: middle;">
                        2024-08-12
                      </td>
                      <td>
                        Presented <strong>ManipVQA</strong> [<a href="#2">2</a>] at
                        <strong>Microsoft Research Asia Tech Fest</strong>
                        (Beijing, China).
                      </td>
                    </tr>
                    <tr>
                      <td style="width: 70px; vertical-align: middle;">
                        2024-06-30
                      </td>
                      <td>
                        Our paper <strong>ManipVQA</strong> [<a href="#2">2</a>] has been accepted
                        for publication at
                        <a href="http://iros2024-abudhabi.org" target="_blank">IROS 2024</a>.
                      </td>
                    </tr>
                  </table>
                  <br />

                  <h2>Research</h2>
                  <p>
                    My research focuses on the intersection of embodied
                    AI, visual perception of the world, reasoning, and
                    robotic control. Specifically, I investigate how to
                    enable embodied agents to obtain environmental
                    awareness through vision, including affordance
                    understanding [<a href="#1">1</a>,
                    <a href="#2">2</a>, <a href="#3">3</a>,
                    <a href="#5">5</a>] and spatial reasoning [<a href="#4">4</a>], in order to
                    perform complex manipulation tasks.
                    Currently, I am investigating these areas using
                    large multimodal foundation models.
                  </p>

                  <h2 style="display: inline;">Selected Publications</h2>
                  <table style="width:100%; border-spacing:10px;">
                    <tr>
                      <td style="width:89%; vertical-align:top;">
                        <span style="float: bottom;">(<strong>*</strong>) indicates equal
                          contribution,
                          while (<strong>†</strong>) denotes the corresponding
                          author</span>
                      </td>
                    </tr>
                  </table>

                  <table style="width:100%;margin:auto;border-collapse: collapse;">
                    <tbody>
                      <!-- ManipGPT - Start -->
                      <tr onmouseout="manipgpt_stop()" onmouseover="manipgpt_start()" bgcolor="#FFFFFF">
                        <td style="padding: 5px 0px 5px 10px; width: 2%; vertical-align: middle;">
                          <p id="5">[5]</p>
                        </td>

                        <td style="padding: 5px 0px 5px 10px; width: 20%; vertical-align: middle;">
                          <div class="one">
                            <div class="two" id="manipgpt_image" style="position:relative;">
                              <video autoplay loop muted style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                <source src type="video/mp4" />
                                Your browser does not support video tags.
                              </video>
                              <img src="assets/data/publications/2024_ManipGPT/ManipGPT_Demo.jpg" style="width:100%;display:block;" />
                            </div>
                          </div>
                        </td>

                        <td style="padding: 5px 10px 5px 10px; width: 78%; vertical-align: top;">
                          <span class="papertitle">ManipGPT: Is Affordance Segmentation by Large
                            Vision Models Enough
                            <br />
                            for Articulated Object
                            Manipulation?</span>
                          <br />

                          <a href="https://scholar.google.com/citations?hl=en&user=aQTkWjQAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Taewhan Kim</a>, Hojin Bae, Zeming Li,
                          <a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Xiaoqi Li</a>,
                          <strong>Iaroslav Ponomarenko</strong>,
                          <br>
                          <a href="https://scholar.google.com/citations?hl=en&user=qVyvE6UAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ruihai Wu</a>,
                          <a href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hao Dong†</a>
                          <br />
                          <br />

                          In Review, 2024
                          <br />
                          <!-- <strong><span style="color: red;">
                              []
                            </span></strong>
                          <br /> -->

                          <a href="https://arxiv.org/abs/2412.10050" target="_blank">
                            arXiv</a>
                        </td>

                        <script type="text/javascript">
                          function manipgpt_start() {
                            document.querySelector('#manipgpt_image video').style.opacity = "1";
                          }
                          function manipgpt_stop() {
                            document.querySelector('#manipgpt_image video').style.opacity = "0";
                          }
                          manipgpt_stop();
                        </script>
                      </tr>
                      <!-- ManipGPT - End -->

                      <!-- SpatialBot - Start -->
                      <tr onmouseout="spatialbot_stop()" onmouseover="spatialbot_start()" bgcolor="#EFF9FE">
                        <td style="padding: 5px 0px 5px 10px; width: 2%; vertical-align: middle;">
                          <p id="4">[4]</p>
                        </td>

                        <td style="padding: 5px 0px 5px 10px; width: 20%; vertical-align: middle;">
                          <div class="one">
                            <div class="two" id="spatialbot_image" style="position:relative;">
                              <video autoplay loop muted style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                <source src="assets/data/publications/2024_SpatialBot/SpatialBot-E_Dataset_Collection_Demo.mp4" type="video/mp4" />
                                Your browser does not support video tags.
                              </video>
                              <img src="assets/data/publications/2024_SpatialBot/SpatialBot_Demo.jpg" style="width:100%;display:block;" />
                            </div>
                          </div>
                        </td>

                        <td style="padding: 5px 10px 5px 10px; width: 78%; vertical-align: top;">
                          <span class="papertitle">SpatialBot: Precise Spatial Understanding with
                            Vision Language Models</span>
                          <br />

                          <a href="https://scholar.google.com/citations?hl=en&user=9K3ox0QAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Wenxiao Cai*</a>, <strong>Iaroslav
                            Ponomarenko*</strong>,
                          <a href="https://scholar.google.com/citations?hl=en&user=BUJPCegAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Jianhao Yuan</a>,
                          <a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Xiaoqi Li</a>,
                          Wankou Yang,
                          <br />
                          <a href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hao Dong</a>,
                          <a href="https://scholar.google.com/citations?user=R3_AR5EAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Bo Zhao†</a>
                          <br />
                          <br />

                          In Review, 2024
                          <br />
                          <!-- <strong><span style="color: red;">
                              []
                            </span></strong>
                          <br /> -->

                          <a href="assets/data/publications/2024_SpatialBot/2024_09_16_SpatialBot_ICRA25_latest.pdf" target="_blank">
                            Paper</a>
                          |
                          <a href="https://github.com/BAAI-DCAI/SpatialBot" target="_blank" style="display: inline-flex; align-items: center; gap: 8px;">
                            GitHub
                            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BAAI-DCAI/SpatialBot?style=social" style="vertical-align: bottom;">
                          </a>
                        </td>

                        <script type="text/javascript">
                          function spatialbot_start() {
                            document.querySelector('#spatialbot_image video').style.opacity = "1";
                          }
                          function spatialbot_stop() {
                            document.querySelector('#spatialbot_image video').style.opacity = "0";
                          }
                          spatialbot_stop();
                        </script>
                      </tr>
                      <!-- SpatialBot - End -->

                      <!-- ManipVQA - Start -->
                      <tr onmouseout="manipvqa_stop()" onmouseover="manipvqa_start()" bgcolor="#EFF9FE">
                        <td style="padding: 5px 0px 5px 10px; width: 2%; vertical-align: middle;">
                          <p id="3">[3]</p>
                        </td>

                        <td style="padding: 5px 0px 5px 10px; width: 20%; vertical-align: middle;">
                          <div class="one">
                            <div class="two" id="manipvqa_image" style="position:relative;">
                              <video autoplay loop muted style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                <source src type="video/mp4" />
                                Your browser does not support video tags.
                              </video>
                              <img src="assets/data/publications/2024_ManipVQA/ManipVQA_Demo.jpg" style="width:100%;display:block;" />
                            </div>
                          </div>
                        </td>

                        <td style="padding: 5px 10px 5px 10px; width: 78%; vertical-align: top;">
                          <span class="papertitle">ManipVQA: Injecting Robotic Affordance and
                            Physically Grounded Information
                            <br>
                            into Multi-Modal Large Language Models</span>
                          <br />
                          <a href="https://scholar.google.com/citations?user=QNkS4KEAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Siyuan Huang*</a>,
                          <strong>Iaroslav
                            Ponomarenko*</strong>,
                          <a href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Zhengkai Jiang</a>,
                          <a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Xiaoqi Li</a>,
                          <a href="https://scholar.google.com/citations?user=3lMuodUAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Xiaobin Hu</a>,
                          <br />
                          <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Peng Gao</a>,
                          <a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hongsheng Li</a>,
                          <a href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hao Dong†</a>

                          <br />
                          <br />

                          <span style="color: red;">
                            [Oral Pitch and Interactive Presentation]
                          </span>
                          <br />
                          International Conference on Intelligent Robots and
                          Systems (<a href="http://iros2024-abudhabi.org" target="_blank">IROS
                            2024</a>)
                          <br />
                          <a href="assets/data/publications/2024_ManipVQA/2024-08-29_ManipVQA_IROS24_latest.pdf" target="_blank">Paper</a>
                          |
                          <a href="https://arxiv.org/abs/2403.11289" target="_blank">arXiv</a>
                          |
                          <a href="https://youtu.be/EMiP9J_J8Sk" target="_blank">Oral Pitch</a>
                          |
                          <a href="assets/data/publications/2024_ManipVQA/IROS24_Oral_Pitch_(slides).pdf" target="_blank">Slides</a>
                          |
                          <a href="assets/data/publications/2024_ManipVQA/IROS24_Poster.pdf" target="_blank">Poster</a>
                          |
                          <a href="https://github.com/SiyuanHuang95/ManipVQA" target="_blank" style="display: inline-flex; align-items: center; gap: 8px;">
                            GitHub
                            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SiyuanHuang95/ManipVQA?style=social">
                          </a>
                          <br />
                        </td>

                        <script type="text/javascript">
                          function manipvqa_start() {
                            document.querySelector('#manipvqa_image video').style.opacity = "1";
                          }
                          function manipvqa_stop() {
                            document.querySelector('#manipvqa_image video').style.opacity = "0";
                          }
                          manipvqa_stop();
                        </script>
                      </tr>
                      <!-- ManipVQA - End -->

                      <!-- ImageManip - Start -->
                      <tr onmouseout="ImageManip_stop()" onmouseover="ImageManip_start()" bgcolor="#FFFFFF">
                        <td style="padding: 5px 0px 5px 10px; width: 2%; vertical-align: middle;">
                          <p id="2">[2]</p>
                        </td>

                        <td style="padding: 5px 0px 5px 10px; width: 20%; vertical-align: middle;">
                          <div class="one">
                            <div class="two" id="ImageManip_image" style="position:relative;">
                              <video autoplay loop muted style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                <source src type="video/mp4" />
                                Your browser does not support video tags.
                              </video>
                              <img src="assets/data/publications/2023_ImageManip/ImageManip_Demo.jpg" style="width:100%;display:block;" />
                            </div>
                          </div>
                        </td>

                        <td style="padding: 5px 10px 5px 10px; width: 78%; vertical-align: top;">
                          <span class="papertitle">ImageManip: Image-based Robotic Manipulation
                            with Affordance-guided
                            <br>
                            Next View Selection</span>
                          <br />
                          <a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Xiaoqi Li</a>, Yanzi Wang,
                          <a href="https://scholar.google.com/citations?hl=en&user=iIs4TDMAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Yan Shen</a>, <strong>Iaroslav Ponomarenko</strong>,
                          <a href="https://scholar.google.com/citations?hl=en&user=wNDTItAAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Haoran Lu</a>,
                          <a href="https://scholar.google.com/citations?hl=en&user=vDzY-hQAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Qianxu Wang</a>,
                          <br />
                          <a href="https://scholar.google.com/citations?hl=en&user=-1ITQkwAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Boshi An</a>,
                          <a href="https://scholar.google.com/citations?hl=en&user=cPki5sUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Jiaming Liu</a>,
                          <a href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hao Dong†</a>
                          <br />
                          <br />

                          <a href="https://arxiv.org/abs/2310.09069" target="_blank">
                            arXiv</a>
                        </td>

                        <script type="text/javascript">
                          function ImageManip_start() {
                            document.querySelector('#ImageManip_image video').style.opacity = "1";
                          }
                          function ImageManip_stop() {
                            document.querySelector('#ImageManip_image video').style.opacity = "0";
                          }
                          ImageManip_stop();
                        </script>
                      </tr>
                      <!-- ImageManip - End -->

                      <!-- LPAVAA3DOM - Start -->
                      <tr onmouseout="LPAVAA3DOM_stop()" onmouseover="LPAVAA3DOM_start()" bgcolor="#FFFFFF">
                        <td style="padding: 5px 0px 5px 10px; width: 2%; vertical-align: middle;">
                          <p id="1">[1]</p>
                        </td>

                        <td style="padding: 5px 0px 5px 10px; width: 20%; vertical-align: middle;">
                          <div class="one">
                            <div class="two" id="LPAVAA3DOM_image" style="position:relative;">
                              <video autoplay loop muted style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                <source src type="video/mp4" />
                                Your browser does not support video tags.
                              </video>
                              <img src="assets/data/publications/2023_LPAVAA3DOM/LPAVAA3DOM_Demo.jpg" style="width:100%;display:block;" />
                            </div>
                          </div>
                        </td>

                        <td style="padding: 5px 10px 5px 10px; width: 78%; vertical-align: top;">
                          <span class="papertitle">Learning Part-Aware Visual Actionable
                            Affordance
                            for 3D Articulated
                            <br>
                            Object Manipulation</span>
                          <br />
                          <a href="https://scholar.google.com/citations?hl=en&user=jOPXmhIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Yuanchen Ju*</a>,
                          <a href="https://scholar.google.com/citations?user=Inr-6rEAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Haoran Geng*</a>, Ming Yang*,
                          <a href="https://scholar.google.com/citations?user=q22ys2QAAAAJ&hl&view_op=list_works&sortby=pubdate" target="_blank">Yiran Geng</a>,
                          <strong>Yaroslav Ponomarenko</strong>,
                          <br />
                          <a href="https://www.linkedin.com/in/taewhan-kim-3122621a7/" target="_blank">Taewhan Kim</a>,
                          <a href="https://scholar.google.com/citations?hl=en&user=roCAWkoAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">He Wang</a>,
                          <a href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate" target="_blank">Hao Dong†</a>
                          <br />
                          <br />

                          <span style="color: red;">
                            [Spotlight Presentation]
                          </span>
                          <br />
                          CVPR Workshop on 3D Vision and Robotics (CVPR @
                          3DVR), 2023
                          <br />
                          <a href="assets/data/publications/2023_LPAVAA3DOM/CVPR_W_2023_Part_Aware_Affordance.pdf" target="_blank">
                            Paper</a>
                          |
                          <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">
                            Workshop</a>
                        </td>

                        <script type="text/javascript">
                          function LPAVAA3DOM_start() {
                            document.querySelector('#LPAVAA3DOM_image video').style.opacity = "1";
                          }
                          function LPAVAA3DOM_stop() {
                            document.querySelector('#LPAVAA3DOM_image video').style.opacity = "0";
                          }
                          LPAVAA3DOM_stop();
                        </script>
                      </tr>
                      <!-- LPAVAA3DOM - End -->
                    </tbody>
                  </table>
                  <br />

                  <h2>Service</h2>
                  <table style="width:100%; border-spacing: 10px;">
                    <tr>
                      <td style="width:89%; vertical-align: top;">
                        Reviewer for the IEEE International Conference on
                        Robotics and Automation (<a href="https://2025.ieee-icra.org" target="_blank">ICRA 2025</a>).
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
  <p style="text-align: center;">
    <!--LAST_UPDATE_START-->
      Last updated on Sunday, January 19, 2025, at 09:31:42 AM.
      <!--LAST_UPDATE_END-->
  </p>
  <p style="text-align:center; font-size:small;">
    Design and source code from
    <a style="font-size:small;" href="https://jonbarron.info" target="_blank">Jon Barron's
      website</a>.
    <br>
  </p>
</body>

</html>
