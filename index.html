<!DOCTYPE html>
<html lang="en">

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Yaroslav Ponomarenko</title>

    <meta name="author" content="Yaroslav Ponomarenko" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  </head>

  <body>
    <table
      style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <h2 class="name" style="text-align: center;">
                      Iaroslav Ponomarenko
                    </h2>

                    <p>
                      <center>
                        yaroslav [dot] ponomarenko [at] stu [dot] pku [dot] edu [dot] cn
                        <!-- yaroslav . ponomarenko @ stu . pku . edu . cn -->
                      </center>
                    </p>

                    <p>
                      I am a second-year master's student at
                      <a href="https://cs.pku.edu.cn">Peking University</a>'s
                      <a href="https://cfcs.pku.edu.cn/people/graduate_students/index.htm">
                        Center on Frontiers of Computing Studies</a>,
                      where I am fortunate to work with Professor
                      <a href="https://zsdonghao.github.io">Hao Dong</a>
                      at the joint
                      <a href="https://cfcs.pku.edu.cn/english/research/researchlabs/237027.htm">
                        Hyperplane/AGIBot PKU Lab
                      </a>
                      on embodied AI, robotics, large foundation models, and
                      computer vision. Additionally, I am a research intern at
                      <a href="https://agibot.com">AGIBot</a>.
                    </p>

                    <p>
                      Previously, I obtained an Engineering degree in
                      Information Systems and Technologies from
                      <a href="https://vivt.ru">Voronezh Institute of High Technologies</a>,
                      as well as a Technician degree in Automated Information
                      Processing and Control Systems from Borisoglebsk College
                      of Informatics and Computer Engineering.
                    </p>

                    <p style="text-align:center">
                      <!-- <a href="data/YaroslavPonomarenko-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                      <!-- <a href="data/YaroslavPonomarenko-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                      <a
                        href="https://scholar.google.com/citations?hl=en&amp;user=bBFYNasAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google
                        Scholar</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/yaroslavponomarenko/">LinkedIn</a>
                      &nbsp;/&nbsp;
                      <a href="https://github.com/YaroslavPonomarenko">GitHub</a>
                    </p>
                  </td>

                  <td style="padding:2.5%;width:40%;max-width:40%">
                    <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;"
                      alt="Iaroslav Ponomarenko" src="data/images/IaroslavPonomarenko.png" />
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">

                    <h2>Research Interest</h2>
                    <p>
                      My research focuses on the intersection of embodied AI, visual perception of
                      the world, reasoning, and robotic control. Specifically, I investigate how to
                      enable embodied agents to obtain environmental awareness through vision,
                      including affordance understanding [<a href="#1">1</a>, <a href="#2">2</a>]
                      and spatial reasoning [<a href="#3">3</a>], in order
                      to perform complex manipulation tasks. Currently, I am investigating these
                      areas using large multimodal foundation models.
                    </p>
                    <br>

                    <h2>News</h2>
                    <table style="width:100%; border-spacing:10px;">
                      <tr>
                        <td style="width:11%; vertical-align:middle;">2024-08-12</td>
                        <td style="width:89%; vertical-align:top;">Presented
                          <strong>ManipVQA</strong> [<a href="#2">2</a>] during the poster session
                          at
                          <strong>Microsoft Research Asia Tech Fest</strong>.
                          <br>
                        </td>
                      </tr>
                      <tr>
                        <td style="width:11%; vertical-align:middle;">2024-06-30</td>
                        <td style="width:89%; vertical-align:top;">ðŸŽ‰ Our paper
                          <strong>ManipVQA</strong> [<a href="#2">2</a>] has
                          been accepted for publication at <strong>IROS 2024</strong>.
                        </td>
                      </tr>
                    </table>
                    <br>
                    <br>

                    <h2 style="display: inline;">Selected Publications</h2><br>
                    <span
                      style="float: bottom;">(<strong>*</strong>) indicates equal contribution,
                      while (<strong>â€ </strong>)
                      denotes
                      the corresponding author</span>

                    <table style="width:100%;border:0;margin:auto;border-spacing:0;">
                      <tbody>

                        <!-- SpatialBot - Start -->
                        <tr onmouseout="spatialbot_stop()" onmouseover="spatialbot_start()" bgcolor>
                          <td
                            style="padding: 20px 10px 20px 20px; width: 5%; vertical-align: middle;">
                            <h4 id="3">[3]</h4>
                          </td>

                          <td
                            style="padding: 20px 10px 20px 10px; width: 25%; vertical-align: middle;">
                            <div class="one">
                              <div class="two" id="spatialbot_image" style="position:relative;">
                                <video autoplay loop muted
                                  style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                  <source
                                    src="data/publications/2024_SpatialBot/SpatialBot-E_Dataset_Collection_Demo.mp4"
                                    type="video/mp4" />
                                  Your browser does not support video tags.
                                </video>
                                <img src="data/publications/2024_SpatialBot/SpatialBot_Demo.jpg"
                                  style="width:100%;display:block;" />
                              </div>
                            </div>
                          </td>

                          <td
                            style="padding: 20px 20px 20px 10px; width: 70%; vertical-align: middle;">
                            <span class="papertitle">SpatialBot: Precise Spatial Understanding with
                              Vision
                              Language Models</span>
                            <br />

                            <a
                              href="https://scholar.google.com/citations?hl=en&user=9K3ox0QAAAAJ&view_op=list_works&sortby=pubdate">Wenxiao
                              Cai*</a>,
                            <strong>Iaroslav Ponomarenko*</strong>,
                            <a
                              href="https://scholar.google.com/citations?hl=en&user=BUJPCegAAAAJ&view_op=list_works&sortby=pubdate">Jianhao
                              Yuan</a>,
                            <a
                              href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Xiaoqi
                              Li</a>,
                            Wankou Yang,
                            <a
                              href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Hao
                              Dong</a>,
                            <a
                              href="https://scholar.google.com/citations?user=R3_AR5EAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Bo
                              Zhaoâ€ </a>
                            <br />

                            In Review, 2024
                            <br>
                            <!-- <strong><span style="color: red;">
                        []
                      </span></strong>
                    <br /> -->

                            <a href="https://github.com/BAAI-DCAI/SpatialBot">GitHub</a>
                            <p>
                              We introduce SpatialBot, a framework specifically designed to improve
                              spatial
                              reasoning of Vision Language Models (VLMs) by leveraging both RGB and
                              depth
                              images. To train VLMs for depth perception, we present the SpatialQA
                              and
                              SpatialQA-E datasets, which feature depth-related questions at
                              multiple levels
                              across various scenarios and embodiment tasks. In addition, we release
                              models
                              fine-tuned on SpatialQA and SpatialQA-E datasets, and present
                              SpatialBench, a
                              comprehensive evaluation framework for assessing spatial understanding
                              capabilities of VLMs.</p>
                          </td>

                          <script type="text/javascript">
                          function spatialbot_start() {
                            document.querySelector('#spatialbot_image video').style.opacity = "1";
                          }
                          function spatialbot_stop() {
                            document.querySelector('#spatialbot_image video').style.opacity = "0";
                          }
                          spatialbot_stop();
                        </script>
                        </tr>
                        <!-- SpatialBot - End -->

                        <!-- ManipVQA - Start -->
                        <tr onmouseout="manipvqa_stop()" onmouseover="manipvqa_start()"
                          bgcolor="#ffffff">
                          <td
                            style="padding: 20px 10px 20px 20px; width: 5%; vertical-align: middle;">
                            <h4 id="2">[2]</h4>
                          </td>

                          <td
                            style="padding: 20px 10px 20px 10px; width: 25%; vertical-align: middle;">
                            <div class="one">
                              <div class="two" id="manipvqa_image" style="position:relative;">
                                <video autoplay loop muted
                                  style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                  <source src type="video/mp4" />
                                  Your browser does not support video tags.
                                </video>
                                <img src="data/publications/2024_ManipVQA/ManipVQA_Demo.jpg"
                                  style="width:100%;display:block;" />
                              </div>
                            </div>
                          </td>

                          <td
                            style="padding: 20px 20px 20px 10px; width: 70%; vertical-align: middle;">
                            <span class="papertitle">ManipVQA: Injecting Robotic Affordance and
                              Physically
                              Grounded Information into Multi-Modal Large Language Models</span>
                            <br />

                            <a
                              href="https://scholar.google.com/citations?user=QNkS4KEAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Siyuan
                              Huang*</a>, <strong>Yaroslav Ponomarenko*</strong>,
                            <a
                              href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Zhengkai
                              Jiang</a>,
                            <a
                              href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Xiaoqi
                              Li</a>,
                            <a
                              href="https://scholar.google.com/citations?user=3lMuodUAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Xiaobin
                              Hu</a>,
                            <a
                              href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Peng
                              Gao</a>,
                            <a
                              href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Hongsheng
                              Li</a>,
                            <a
                              href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Hao
                              Dongâ€ </a>
                            <br />

                            International Conference on Intelligent Robots and Systems (IROS), 2024
                            <br>
                            <strong><span style="color: red;">
                                [Oral Pitch and Interactive Presentation]
                              </span></strong>
                            <br />

                            <a
                              href="data/publications/2024_ManipVQA/2024-08-29_ManipVQA_IROS24_final.pdf">
                              Final Version</a>
                            /
                            <a href="https://arxiv.org/pdf/2403.11289v2">arXiv</a>
                            /
                            <a href="https://github.com/SiyuanHuang95/ManipVQA">GitHub</a>
                            <p>
                              We introduce ManipVQA, a unified VQA format training
                              dataset, model, and a robust framework for integrating
                              physical knowledge and affordance reasoning into
                              multimodal large language models (MLLMs). </p>
                          </td>

                          <script type="text/javascript">
                          function manipvqa_start() {
                            document.querySelector('#manipvqa_image video').style.opacity = "1";
                          }
                          function manipvqa_stop() {
                            document.querySelector('#manipvqa_image video').style.opacity = "0";
                          }
                          manipvqa_stop();
                        </script>
                        </tr>
                        <!-- ManipVQA - End -->

                        <!-- LPAAV3DOM - Start -->
                        <tr onmouseout="LPAAV3DOM_stop()" onmouseover="LPAAV3DOM_start()"
                          bgcolor="#FFFFFF">
                          <td
                            style="padding: 20px 10px 20px 20px; width: 5%; vertical-align: middle;">
                            <h4 id="1">[1]</h4>
                          </td>

                          <td
                            style="padding: 20px 10px 20px 10px; width: 25%; vertical-align: middle;">
                            <div class="one">
                              <div class="two" id="LPAAV3DOM_image" style="position:relative;">
                                <video autoplay loop muted
                                  style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                                  <source src type="video/mp4" />
                                  Your browser does not support video tags.
                                </video>
                                <img src="data/publications/2023_LPAAV3DOM/LPAAV3DOM_Demo.jpg"
                                  style="width:100%;display:block;" />
                              </div>
                            </div>
                          </td>

                          <td
                            style="padding: 20px 20px 20px 10px; width: 70%; vertical-align: middle;">
                            <span class="papertitle">Learning Part-Aware Visual Actionable
                              Affordance for
                              3D Articulated Object Manipulation</span>
                            <br />
                            <a
                              href="https://scholar.google.com/citations?hl=en&user=jOPXmhIAAAAJ&view_op=list_works&sortby=pubdate">Yuanchen
                              Ju*</a>,
                            <a
                              href="https://scholar.google.com/citations?user=Inr-6rEAAAAJ&view_op=list_works&sortby=pubdate">Haoran
                              Geng*</a>, Ming Yang*,
                            <a
                              href="https://scholar.google.com/citations?user=q22ys2QAAAAJ&hl&view_op=list_works&sortby=pubdate">Yiran
                              Geng</a>, <strong>Yaroslav Ponomarenko</strong>,
                            <a href="https://www.linkedin.com/in/taewhan-kim-3122621a7/">Taewhan
                              Kim</a>,
                            <a
                              href="https://scholar.google.com/citations?hl=en&user=roCAWkoAAAAJ&view_op=list_works&sortby=pubdate">He
                              Wang</a>,
                            <a
                              href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en&view_op=list_works&sortby=pubdate">Hao
                              Dongâ€ </a>
                            <br />
                            CVPR Workshop on 3D Vision and Robotics (CVPR @ 3DVR), 2023 <br>
                            <strong><span style="color: red;">
                                [Spotlight Presentation]
                              </span></strong>
                            <br />
                            <a
                              href="https://drive.google.com/file/d/1ESca7sAJ4CsH3R6H33shp0iodlp1OlKw/view?usp=drive_link">Paper</a>
                            /
                            <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics">
                              Workshop</a>
                            <p>
                              We introduce Part-aware Affordance Learning methods. Our
                              approach initially learns a prior for object parts and
                              then generates an affordance map. To further improve
                              precision, we incorporate a part-level scoring system to
                              identify the most suitable part for manipulation. </p>
                          </td>

                          <script type="text/javascript">
                            function LPAAV3DOM_start() {
                            document.querySelector('#LPAAV3DOM_image video').style.opacity = "1";
                          }
                          function LPAAV3DOM_stop() {
                            document.querySelector('#LPAAV3DOM_image video').style.opacity = "0";
                          }
                          LPAAV3DOM_stop();
                        </script>
                        </tr>
                        <!-- LPAAV3DOM - End -->
                      </tbody>
                    </table>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
    <p style="text-align:center;font-size:small;">
      <!--LAST_UPDATE_START-->
      Last updated on Monday, September 30, 2024, at 11:09:58 AM.
      <!--LAST_UPDATE_END-->
    </p>
  </body>

  <p style="text-align:center;font-size:small;">
    Design and source code from
    <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>.
  </p>
